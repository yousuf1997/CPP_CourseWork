{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2febe58",
   "metadata": {},
   "source": [
    "# CS4650 Topic 9: Map Reduce\n",
    "\n",
    "There are several ways to run Map-Reduce projects.  One of the interesting ways is using a Jupyter Notebook.  Notebooks provide a really nice way to experiment, try things, then document the results.  In fact, this lecture was written using a Notebook, and there were a lot of iterations as I was experimenting with the code, adjusting the mappers and reducers. \n",
    "\n",
    "Another reason for iterations was that, as I looked at the results, I thought of new questions to ask, new calculations to explore.\n",
    "  \n",
    "Usually when you iterate in a Notebook, you go back to a Cell and change things a bit, then rerun.  I didn't do that much, rather I duplicated Cells, so that you could see the progression of my experiments.  Sometimes this meant that I was subtlely changing function names, so there wouldn't be collisions.\n",
    "\n",
    "## MRJob\n",
    "\n",
    "There is a very useful package called MRJob.  I refer to it as 'Mister Job', but it really means 'Map-Reduce Job'.\n",
    "\n",
    "With MRJob, you write one function that is the Mapper, another which is the Reducer (and you can also write a Combiner).  You can then run this on a dataset on your computer, right within Notebook.  Or you can connect to the web and run the job on a cluster of computers.  The local machine is usually running sample data, used to debug the code, while the cluster is used for the production run.\n",
    "\n",
    "To install mrjob, activate the environment to be used, then in the Terminal enter the following:\n",
    "\n",
    "```\n",
    "conda install -c conda-forge mrjob\n",
    "```\n",
    "\n",
    "The '-c conda-forge' is telling conda where to search for the mrjob package.  Conda has a site with many packages, and this is the default place to find packages.  However, mrjob is not in that default site.\n",
    "\n",
    "We will see how to use mrjob in just a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867791a",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "\n",
    "For our first example, let's write a program to scan through a large text document, finding all words in that document, then counting how many times each of those words were used.\n",
    "\n",
    "You may want to create a new directory and a new environment for this project, since we are going to be downloading extra files, and you don't want to clutter up your workspace by having files from different projects all in the same place.\n",
    "\n",
    "In any event, you will want to create a new Jupyter Notebook.  I've called mine _word-count_.\n",
    "\n",
    "Let's talk about the source of data for our project:\n",
    "\n",
    "There is a website, _www.gutenberg.org_, that collects public-domain books, providing these for free.  They've got over 60,000 books.  The books are available in a number of formats, we will use the UTF-8 text format.\n",
    "\n",
    "These are text documents, so each is just the words.  The old ASCII format is not sufficient, even though all English words can be expressed in ASCII, words from other languages use characters other than A-Z.  Hence, Unicode is used for these books, and UTF-8 is a convenient way to represent Unicode characters.\n",
    "\n",
    "I've chosen 3 books to use:\n",
    "\n",
    "1. Alice's Adventures in Wonderland\n",
    "\n",
    "2. The Adventures of Sherlock Holmes\n",
    "\n",
    "3. The Count of Monte Cristo\n",
    "\n",
    "Feel free to experiment on your own, to download different books!\n",
    "\n",
    "To download _Alice's Adventures in Wonderland_, browse to\n",
    "\n",
    "```\n",
    "https://www.gutenberg.org/files/11/11-0.txt\n",
    "```\n",
    "\n",
    "This will send the text to your browser.  You can then save the page to a file.  Save it to the file _alice.txt_, in your current project folder.\n",
    "\n",
    "To download _The Adventures of Sherlock Holmes_, browse to the following, then save the results to the file _sherlock.txt_.\n",
    "\n",
    "```\n",
    "https://www.gutenberg.org/files/1661/1661-0.txt\n",
    "```\n",
    "\n",
    "Finally, to download _The Count of Monte Cristo_, browse to the following, then save the results to the file _montecristo.txt_.\n",
    "\n",
    "```\n",
    "https://www.gutenberg.org/cache/epub/1184/pg1184.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a66499",
   "metadata": {},
   "source": [
    "# Writing the Mapper and Reducer\n",
    "\n",
    "We are now going to write some Python code, which will be the mapper and the reducer for our MapReduce job.\n",
    "\n",
    "However, this has to be in a separate file, it cannot be in the Notebook itself.\n",
    "\n",
    "Why?  Well, if we are going to run the MapReduce on a cluster of computers, then the code for the Mapper and Reducer has to be farmed out to a bunch of computers.\n",
    "\n",
    "The system could just take the code from one Cell of the Notebook, but maybe the code would extend beyond just one Cell.  And recall that we can define variables and functions in one Cell, but then use these values in other Cells.  So how would the system know which Cells to grab code from for each of the other machines?\n",
    "\n",
    "Consequently, the code has to be in a separate, stand-alone file.\n",
    "\n",
    "However, there is an interesting trick that can be employed: You can have a Cell in the Notebook _create the Python file_.  In this way, the Notebook is self-contained.  If the Notebook is shared, the Notebook itself will create that Python file, then use it for the MapReduce job.\n",
    "\n",
    "There is a special feature of Notebook which allows a Cell to create a file from the contents of the Cell.  Notebook has several features like this, these are called _magic_ commands.\n",
    "\n",
    "In this case, we will start the Cell (a Code Cell) with the following line:\n",
    "\n",
    "```\n",
    "%%file word_count.py\n",
    "```\n",
    "\n",
    "This magic line indicates that the contents of the cell (following this magic line) are to be written to the named file.  Later we will see mrjob use this file.\n",
    "\n",
    "Note that if we re-run this Cell, that file will get re-written.  That is OK!\n",
    "\n",
    "There is a second trick, which is commonly used, appearing at the end of this Cell.  Here is that code:\n",
    "\n",
    "```\n",
    "if __name__ == '__main__':\n",
    "    WordFrequency.run()\n",
    "```\n",
    "\n",
    "That 'WordFrequency' is the class that we will be defining in this Cell.  The above code checks to see if this is being executed as a Cell being run, and if so, it runs an instance of our mrjob subclass.  But later we will discuss what happens if we are executing this on a cluster.  For now, though, we will simply add this to the bottom of the Cell.\n",
    "\n",
    "The following Cell is the complete implementation of the first version of our mapper/reducer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file word_count.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class WordFrequency(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        thelist = line.split()\n",
    "        for x in thelist:\n",
    "            yield x, 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    WordFrequency.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c912896",
   "metadata": {},
   "source": [
    "If you are running this Notebook on your computer, be sure to run that Cell, so that the file will be created.\n",
    "\n",
    "_Usually when 'running' a Notebook, you run each of the Cells in order._\n",
    "\n",
    "We will now have a second Cell which runs that newly created Python file:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;_Note: the following cell works on my Windows computer.  But on my Mac, the line looks a little different:_\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;_!python word_count.py -r local alice.txt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5882b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python word_count.py alice.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e25a96",
   "metadata": {},
   "source": [
    "Be sure to run that Cell as well.\n",
    "\n",
    "Congratulations, you just ran a MapReduce job!\n",
    "\n",
    "You can experiment with this a bit, changing it to run Sherlock.txt or montecristo.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea4302",
   "metadata": {},
   "source": [
    "## Analyzing the Results\n",
    "\n",
    "This code ran fine, and we have some results.  \n",
    "\n",
    "Later we will look at other ways to present the results.\n",
    "\n",
    "But first we need to do some clean-up.\n",
    "\n",
    "As I was looking through the results, I found some problems with the data, there were some unfortunate issues with the data:\n",
    "\n",
    "* If a word is capitalized some places, and not others, that would count as two separate words.  But we would like to ignore the capitalization, we want them to count as one word.\n",
    "\n",
    "* There are some punctuation characters that are cluttering up the data.  I see, for example, that 'reply', 'reply,', and 'reply.' are in there, being counted as 3 separate words.\n",
    "\n",
    "* Some compound words include hyphens.  These are a little problematic -- if we simply throw the hyphens away, then the two words are joined into one word, which will look a little odd.  If we convert the hyphens to spaces, the words will be split into two separate words.\n",
    "\n",
    "* There are also Unicode characters appearing in the words.  These show up as '\\u' followed by 4 hexadecimal characters (which are 0-9 and A-F).\n",
    "\n",
    "* The start of the file has some extra characters and so on, which aren't actually part of the document.\n",
    "\n",
    "The problems here are specific to this dataset and this project.  For other situations, the data cleanup phase may not be required, or may have different issues.  The general concept is that we may need to do some cleanup of the data.\n",
    "\n",
    "And there are a number of options:\n",
    "\n",
    "* We can ignore the problem.\n",
    "\n",
    "* We can discard entries that are incorrect.\n",
    "\n",
    "* We can try to fix the problems.\n",
    "\n",
    "Let's try to fix at least some of these problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23ae92",
   "metadata": {},
   "source": [
    "## Capitalization\n",
    "\n",
    "The capitalization problem is pretty easy to fix.  In the mapper, we are looking at one line of the input.  For this version of the mapper, we have split the line into individual _tokens_.  Each of these tokens are a String, and the String class knows how to generate a lowercase version of itself.  Thus, we will simply convert all strings to lowercase, then 'box' and 'Box' (and 'bOX') would all be considered the same word.  This will also convert 'Fred' to 'fred', but we can live with that.\n",
    "\n",
    "We can edit the mapper function as follows:\n",
    "\n",
    "```\n",
    "    def mapper(self, _, line):\n",
    "        thelist = line.split()\n",
    "        for x in thelist:\n",
    "            y = x.lower()\n",
    "            yield y, 1\n",
    "```\n",
    "\n",
    "You can go back a couple of Cells and make this change to the mapper.  Then rerun that Cell and also run the Cell that calls Python to see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb805f",
   "metadata": {},
   "source": [
    "## Punctuation\n",
    "\n",
    "The next issue deals with punctuation characters, such as period and comma.  These are included in some tokens, and will confuse our system.  We have a couple of options here:\n",
    "\n",
    "* We can check to see if the token only has alpha characters, a-z.  If the token has any other characters, we can ignore it.  The following code would do this:\n",
    "\n",
    "```\n",
    "    def mapper(self, _, line):\n",
    "        thelist = line.split()\n",
    "        for x in thelist:\n",
    "            y = x.lower()\n",
    "            if y.isalpha():\n",
    "                yield y, 1\n",
    "```\n",
    "\n",
    "* We can eliminate all characters from the token that are not letters.  This will remove all of the punctuation, but it might lead to other issues.  For example, a unicode-non-ascii character such as \\u201c.  Sometimes these are fancy quote characters, so removing them would be fine.  In other cases, these might be a letter with a diacritical.  Removing them would remove the letter.  If we are going to use this approach, we can use regular expressions to filter out the non-letters, as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e174e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file word_count.py\n",
    "import re\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "PUNC_RE = re.compile(r\"[^a-z]\")\n",
    "\n",
    "class WordFrequency(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        thelist = line.split()\n",
    "        for x in thelist:\n",
    "            y = x.lower()\n",
    "            z = re.sub(PUNC_RE, '', y)\n",
    "            yield z, 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    WordFrequency.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499087d",
   "metadata": {},
   "source": [
    "For convenience, I've replicated the Cell to actually run the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python word_count.py alice.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9895f1",
   "metadata": {},
   "source": [
    "## Compound Words, Unicode\n",
    "\n",
    "Since we are adopting the plan of removing non-alphabetic characters, we have removed hyphens, so compound words are now just run together, and we have removed unicode characters, which may have removed important letters.\n",
    "\n",
    "If these were important issues, we could write other 'rules' to handle these characters.\n",
    "\n",
    "For example, if we wanted to replace hyphens with spaces, so the compound words would simply become different words, we could replace '-' with ' ', before we do the split of the line.\n",
    "\n",
    "For the unicode example, we could search for critical unicode characters, then replace them with the roughly equivalent ASCII character.  This can be done after the split, but should be done before we remove the punctuation.\n",
    "\n",
    "## Introductory Matter\n",
    "\n",
    "If you look at the text files we downloaded, you will see that the first several lines of the file are not actually part of the book, but rather provide an introduction to the book.  This lists the author, date of publication, and so on.\n",
    "\n",
    "This material ends at a line starting with \"*** START \".\n",
    "\n",
    "We can filter this portion of the file by declaring an instance variable for the class, initializing the value to false.  Then, as the mapper scans a line, it checks to see if the line starts with \"*** START\".  If so, it will set that flag to be true.  If the flag was false, we then do not yield any data.\n",
    "\n",
    "There is a problem with this, however.  When we distribute the job across a cluster of computers, each processor will have a subset of the data, a portion of the file.  It is only the processor that is processing the very first part of the data that should do this check.  All of the other processors _should not_ do this check, because then they will never see the \"*** START\", so they will never 'turn on'.\n",
    "\n",
    "Consequently, for our purposes, we will ignore this, and so the introductory matter will also be added to the word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2865ad",
   "metadata": {},
   "source": [
    "# In-Class Exercise\n",
    "\n",
    "Suppose we want to do a different calculation:  Now we want to count the number of words of each length, greater than 3.  So in other words, how many 4-letter words, how many 5-letter words, and so on.  In addition, we want to keep an example of one of the words of that length.  So the output might say:\n",
    "\n",
    "```\n",
    "    For 4 letter words, there were 125 of them, and one was \"stop\"\n",
    "    For 5 letter words, there were 1264 of them, and one was \"right\"\n",
    "```\n",
    "\n",
    "Actually, the output would actually be closer to something like this:\n",
    "\n",
    "```\n",
    "    4, \"stop\", 125\n",
    "    5, \"right\", 1264\n",
    "```\n",
    "\n",
    "What would the mapper look like, and what would the reducer look like, for this application?\n",
    "\n",
    "?\n",
    "\n",
    "?\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c363d2",
   "metadata": {},
   "source": [
    "## The Mapper\n",
    "\n",
    "In the previous example, for each word the mapper found, it output the word, along with a count of 1, because it had just seen one instance of that word.  The word was the key, and the count was the value.  Because the word was the key, the shuffle, combine, and reduce gathered all of the data for each word (the key), then output a value.\n",
    "\n",
    "For our new problem, the key will actually be the _length_ of the word, because we want to aggregate and count things based on this length.  But we then need to output _two_ values, the word and the count.\n",
    "\n",
    "So in the yield, we want to send three things: length, word, and count.  But MapReduce is set to only have two things, a key and a value.\n",
    "\n",
    "To solve this, we will use the length as the key, but for the value we will make an object that contains two values.  The object will have an example word and it will have a count.  Since we will be communicating these values through streams, as text, we need an object that can be serialized as JSON.  If we use a Python dictionary, the object will be automatically serialized as JSON, and at the other end, automatically reconstructed as a dictionary.\n",
    "\n",
    "The output code for the mapper will therefore look like this:\n",
    "\n",
    "```\n",
    "                yield length, {\"example\":word, \"count\":1}\n",
    "```\n",
    "\n",
    "The mapper returns two things, the key (which is the length), and a dictionary with two values, an example word and the count.\n",
    "\n",
    "In the previous example, the reducer received keys, which were the words, and a list of counts.  It simply summed up all of the counts, then emitted the key (word) and the total count.\n",
    "\n",
    "For the new problem, the reducer receives a key, which is the length of the word, and a list of dictionaries.  The reducer will take one of the example words (probably either the first or the last), and it will sum up the counts.  It will then return a key, which is the same length as was in the input, and a value, which is a dictionary holding an example word and the total count for that word.\n",
    "\n",
    "We will change the name of the file, since we are computing a different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4315270",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file word_length_count.py\n",
    "import re\n",
    "import json\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "PUNC_RE = re.compile(r\"[^a-z]\")\n",
    "\n",
    "class WordLengthFrequency(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        thelist = line.split()\n",
    "        for x in thelist:\n",
    "            y = x.lower()\n",
    "            z = re.sub(PUNC_RE, '', y)\n",
    "            w = len(z)\n",
    "            if w > 3:\n",
    "                yield w, {\"example\":z, \"count\":1}\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        example = ''\n",
    "        count = 0\n",
    "        for x in values:\n",
    "            example = x[\"example\"]\n",
    "            count += x[\"count\"]\n",
    "        if len(example) > 0:\n",
    "            yield key, {\"example\":example, \"count\":count}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    WordLengthFrequency.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d421291",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python word_length_count.py alice.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a793e45",
   "metadata": {},
   "source": [
    "Look carefully at the results of this run.  It looks like many of the longest words are actually related to the website from which the books were downloaded.  And also note that the counts of these length of words is very small, usually 1 or 2.  It looks like we should have spent the effort to remove the introductory matter from the books!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ebeec",
   "metadata": {},
   "source": [
    "# Weather Data\n",
    "\n",
    "We will look at one additional example of using MapReduce in a Notebook.  The problem we will solve came from our Hadoop textbook, from Chapter 2.  This example looks through weather data, looking for the largest temperature for each year.\n",
    "\n",
    "In one appendix of the book, they gave instructions for downloading two years worth of data, which we can use as our sample data.  This is the weather data for 1901 and 1902.\n",
    "\n",
    "The code we will use here is a little different from the example code in the book, primarily because we are using Python instead of Java.  However, we are also using mrjob, which interfaces to Hadoop for us, rather that using Hadoop directly.\n",
    "\n",
    "We could have used Hadoop in this class.  I did download Hadoop to my computer, and was setting it up.  But I realized it would take a lot more effort and a lot more disk space.  So we are using Notebooks.  The concepts you learn will apply whichever approach you use in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a95a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file max_temperature.py\n",
    "import re\n",
    "import json\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "QUALITY_RE = re.compile(r\"[01459]\")\n",
    "\n",
    "class MaxTemperature(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        val = line.strip()\n",
    "        (year, temp, q) = (val[15:19], val[87:92], val[92:93])\n",
    "        if (temp != \"+9999\" and re.match(QUALITY_RE, q)):\n",
    "            yield year, int(temp)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, max(values)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MaxTemperature.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python max_temperature.py --no-bootstrap-mrjob 1901 1902"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29bc38",
   "metadata": {},
   "source": [
    "Now let's try to get these values for each month rather than just one number per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file max_monthly_temperature.py\n",
    "import re\n",
    "import json\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "QUALITY_RE = re.compile(r\"[01459]\")\n",
    "\n",
    "monthName = [\"Jan-\", \"Feb-\", \"Mar-\", \"Apr-\", \"May-\", \"Jun-\",\n",
    "            \"Jul-\", \"Aug-\", \"Sep-\", \"Oct-\", \"Nov-\", \"Dec-\"];\n",
    "\n",
    "class MaxMonthlyTemperature(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        val = line.strip()\n",
    "        (year, month, temp, q) = (val[15:19], val[20:21], val[87:92], val[92:93])\n",
    "        if (temp != \"+9999\" and re.match(QUALITY_RE, q)):\n",
    "            yield monthName[int(month) - 1] + str(year), int(temp)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, max(values)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MaxMonthlyTemperature.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8fbae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python max_monthly_temperature.py 1901 1902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bdc97c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
