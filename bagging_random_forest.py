#-------------------------------------------------------------------------
# AUTHOR: Mohamed Yousuf Abdul Rahman
# FILENAME: bagging_random_forest
# SPECIFICATION: The program demonstrates the random forest's bagging algorithm
# FOR: CS 5990- Assignment #4
# TIME SPENT: 3 hrs
#-----------------------------------------------------------*/

#importing some Python libraries
from sklearn import tree
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier

dbTraining = []
dbTest = []
X_training = []
y_training = []
classVotes = [] #this array will be used to count the votes of each classifier

#reading the training data from a csv file and populate dbTraining
with open('optdigits.tra', 'r') as training_file:
    for index, training_data_row in enumerate(csv.reader(training_file)):
        db_training.append(training_data_row)

#reading the test data from a csv file and populate dbTest
with open('optdigits.tes', 'r') as testing_file:
    for index, test_data_row in enumerate(csv.reader(testing_file)):
        db_test.append(test_data_row)

#inititalizing the class votes for each test sample. Example: classVotes.append([0,0,0,0,0,0,0,0,0,0])
for index, test_set in enumerate(dbTest):
    classVotes.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

print("Started my base and ensemble classifier ...")

for k in range(20): #we will create 20 bootstrap samples here (k = 20). One classifier will be created for each bootstrap sample

  bootstrapSample = resample(dbTraining, n_samples=len(dbTraining), replace=True)
  total_correct = 0 ## accuracy count in the modal

  #populate the values of X_training and y_training by using the bootstrapSample
  X_training = []
  for sample in bootstrapSample:
      X_training.append(sample[:-1])

  Y_training = []
  for sample in bootstrapSample:
      Y_training.append(sample[-1])

  #fitting the decision tree to the data
  clf = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth=None) #we will use a single decision tree without pruning it
  clf = clf.fit(X_training, y_training)

  for i, test_sample in enumerate(dbTest):

      #make the classifier prediction for each test sample and update the corresponding index value in classVotes. For instance,
      # if your first base classifier predicted 2 for the first test sample, then classVotes[0,0,0,0,0,0,0,0,0,0] will change to classVotes[0,0,1,0,0,0,0,0,0,0].
      # Later, if your second base classifier predicted 3 for the first test sample, then classVotes[0,0,1,0,0,0,0,0,0,0] will change to classVotes[0,0,1,1,0,0,0,0,0,0]
      # Later, if your third base classifier predicted 3 for the first test sample, then classVotes[0,0,1,1,0,0,0,0,0,0] will change to classVotes[0,0,1,2,0,0,0,0,0,0]
      # this array will consolidate the votes of all classifier for all test samples
      #--> add your Python code here

      predicted_value = clf.predict(X=[test_sample[:-1]])[0]

      ## updating the class votes
      class_votes[i][int(predicted_value)] += 1

      y_test = test_sample[-1]

      if k == 0: #for only the first base classifier, compare the prediction with the true label of the test sample here to start calculating its accuracy
         if predicted_value == y_test:
             total_correct = total_correct + 1

  if k == 0: #for only the first base classifier, print its accuracy here
     #--> add your Python code here
     accuracy = total_correct / (len(db_test) + 1)
     print("Finished my base classifier (fast but relatively low accuracy) ...")
     print("My base classifier accuracy: " + str(accuracy))
     print("")


  #now, compare the final ensemble prediction (majority vote in classVotes) for each test sample with the ground truth label to calculate the accuracy of the ensemble classifier (all base classifiers together)
  #--> add your Python code here
  ## resetting the total_correct
  total_correct = 0
  for index, sample_votes in enumerate(class_votes):
    ## index of the max element
    predicted_class = sample_votes.index(max(sample_votes))
    ## comparing the class to the test data
    if predicted_class == int(db_test[i][-1]):
            total_correct += 1

  #printing the ensemble accuracy here
  accuracy = total_correct / (len(db_test) + 1)
  print("Finished my ensemble classifier (slow but higher accuracy) ...")
  print("My ensemble accuracy: " + str(accuracy))
  print("")

  print("Started Random Forest algorithm ...")

  #Create a Random Forest Classifier
  clf=RandomForestClassifier(n_estimators=20) #this is the number of decision trees that will be generated by Random Forest. The sample of the ensemble method used before

  #Fit Random Forest to the training data
  clf.fit(X_training,y_training)

  #make the Random Forest prediction for each test sample. Example: class_predicted_rf = clf.predict([[3, 1, 2, 1, ...]]
  #--> add your Python code here
  total_correct = 0
  for i, test_sample in enumerate(db_test):
        y_test = test_sample[-1]
        random_forest_prediction = clf.predict([test_sample[:-1]])[0]
        if random_forest_prediction == y_test:
            total_correct += 1
  #compare the Random Forest prediction for each test sample with the ground truth label to calculate its accuracy
  #--> add your Python code here

  #printing Random Forest accuracy here
  accuracy = total_correct / (len(db_test) + 1)
  print("Random Forest accuracy: " + str(accuracy))

  print("Finished Random Forest algorithm (much faster and higher accuracy!) ...")
